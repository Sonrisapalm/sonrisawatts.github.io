<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Article Two — Emotion Encoded</title>
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="style.css" />
</head>
<body>

  <nav>
   <nav class="absolute bottom-0 left-0 right-0 py-4 px-6 md:px-12 flex flex-wrap justify-center md:space-x-8 space-x-4 text-base md:text-xl lg:text-2xl">
            <a href="index.html" class="text-white hover:text-orange-400 transition-colors duration-200 transform hover:scale-105 glow-hover">Home</a>
            <a href="about.html" class="text-white hover:text-orange-400 transition-colors duration-200 transform hover:scale-105 glow-hover">About</a>
            <a href="Methodsanddata.html" class="text-white hover:text-orange-400 transition-colors duration-200 transform hover:scale-105 glow-hover">Methods and Data</a>
            <a href="ethicalguidelines.html" class="text-white hover:text-orange-400 transition-colors duration-200 transform hover:scale-105 glow-hover">Ethical Guidelines</a>
            <a href="AI-Perception-Briefs.html" class="text-white hover:text-orange-400 transition-colors duration-200 transform hover:scale-105 glow-hover">Research Findings Articles</a>
            <a href="contact.html" class="text-white hover:text-orange-400 transition-colors duration-200 transform hover:scale-105 glow-hover">Contact</a>
        </nav>
    </header>
  <header>
    <h1>Conducting Research Continues</h1>
    <p>Insights from the latest Emotion Encoded study.</p>
  </header>

  <main>
    <article>
      <img src="images/articletwo.jpg" alt="Article Two Illustration" />

      <p>
        Emotion Encoded asked participants "Would you take advice from an AI if it knew your entire personality and history?"<br/>
        Majority of people said "Maybe".<br/>
        15.6% said "No, that's creepy."
      </p>

      <p>
        This suggests that people are placing a limit on the amount and type of data that AI would have about them. Imagine an AI bot that knows information about you that you have never known yourself or details that are sacred to you and your family. For some, this idea feels intrusive and unsettling, even if the AI’s advice could potentially improve their decision-making or health outcomes.
      </p>

      <p>
        Some people are completely against the idea of relying on artificial intelligence. Many admit that they could never adapt to a new reality centered around advanced technology. Not even transparency or clear explanations can change their minds. This reaction may stem from status quo bias. Status quo bias is the tendency to prefer existing norms, processes, or decisions even when there are better alternatives. For these individuals, innovation feels threatening because it disrupts long-standing traditions and ways of life.
      </p>

      <p>
        Consider how this plays out in industries. In healthcare, AI could offer early detection of diseases by analyzing scans more quickly than doctors. Yet some patients would still refuse AI-driven recommendations because they prefer advice from a human doctor they know and trust. In law, AI could help lawyers sift through thousands of cases to strengthen arguments, but some professionals resist because they fear losing the human judgment that defines legal reasoning. In finance, AI can model risks and predict market trends, but many investors are skeptical of algorithms, worrying that the technology strips away the personal relationship they have with their advisors.
      </p>

      <p>
        These examples reveal that resistance is not always about the performance of AI systems. Instead, it is about human comfort and cultural attachment to existing methods. Status quo bias reinforces the idea that even when an option is objectively better, people will still prefer what feels familiar and controllable. This resistance can slow down the adoption of new technologies, even in areas where the benefits are clear.
      </p>

      <p>
        What does this mean for AI? It means that developers and policymakers must account for the psychology of human decision-making, not just the technical efficiency of their systems. Building systems that explain their reasoning, allow for human input, and respect personal boundaries around data may increase acceptance. Yet, there will always be groups who refuse to engage with AI, and this human factor will shape how quickly or slowly the technology becomes part of daily life.
      </p>
    </article>
  </main>

  <footer>
    © 2025 Emotion Encoded — All rights reserved.
  </footer>

</body>
</html>
