<main>
  <section class="full-width-bg">
    <article class="article-card">
      <img src="images/article3.jpg" alt="AI Trust Survey Illustration" class="article-img" />

      <h2>ğŸ‘ St. Kitts & Nevis ğŸ‘</h2>
      <h3>Results On What People Need to Trust AI</h3>

      <p>In Emotion Encoded survey, I asked 66 people:</p>
      <blockquote>â€œWhat would you need to know about an AI tool in order to trust it more?â€</blockquote>

      <p>The answers reveal that trust in AI is based on clarity and accountability... and conditions:</p>

      <ul>
        <li>47% want information about the data it was trained on</li>
        <li>45.5% need details about how the AI works</li>
        <li>45.5% also want to know how often it makes mistakes</li>
        <li>43.9% said they would trust AI more if a human was still involved</li>
        <li>Only 21.2% cared most about who created the AI</li>
        <li>A minority (22.7%) admitted they could â€œnever fully trust AIâ€</li>
      </ul>

      <p>
        Influential Tech Giants and powerful people have weaponised 'Explainable AI' (XAI) to reduce the fear of the 'black box' theory in AI trust. However, it's deeper than just explaining decisions solely from their algorithms.
      </p>

      <p>
        From a psychological perspective, this shows that trust is less about liking AI and more about reducing uncertainty. People feel safer when they know how systems make decisions, how often they fail, and whether a human is still in the loop.
      </p>

      <p>
        The findings echo a key truth: trust in AI is built the same way human trust is builtâ€”through transparency, accountability, and reliability. Without these, even the most statistically accurate tools risk rejection.
      </p>
    </article>
  </section>
</main>
