<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Article 3 | Emotion Encoded</title>

  <!-- Tailwind CSS -->
  <script src="https://cdn.tailwindcss.com"></script>

  <!-- Poppins Font -->
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600;700&display=swap" rel="stylesheet" />

  <style>
    :root {
      --primary-color: #f97316; /* orange-500 */
    }

    body {
      font-family: 'Poppins', sans-serif;
      padding-bottom: 7rem; /* space for fixed nav */
    }

    .hero-image {
      width: 100%;
      height: 45vh; /* taller hero */
      object-fit: cover;
      display: block;
    }

    .glow-hover {
      transition: all 0.3s ease;
      position: relative;
    }

    .glow-hover:hover {
      color: var(--primary-color);
      text-shadow: 0 0 10px rgba(249, 115, 22, 0.7),
                   0 0 20px rgba(249, 115, 22, 0.5);
    }

    .article-content {
      max-width: 768px;
    }
  </style>
</head>

<body class="bg-gray-900 text-gray-200 antialiased">

  <!-- HEADER -->
  <header class="mb-8">
    <div class="overflow-hidden">
      <img
        src="images/article3.jpg"
        onerror="this.onerror=null;this.src='https://placehold.co/1200x450/1f2937/d1d5db?text=AI+Trust+Survey+Illustration';"
        alt="AI Trust Survey Illustration"
        class="hero-image rounded-b-3xl shadow-2xl transition-transform duration-500 hover:scale-[1.03]"
      />
    </div>

    <div class="px-4 md:px-12 mt-8 text-center">
      <h1 class="text-4xl sm:text-5xl lg:text-6xl font-extrabold text-white mb-2 leading-tight">
        <span class="text-orange-400">üëè St. Kitts & Nevis üëè</span>
      </h1>
      <p class="text-xl sm:text-2xl text-gray-400">
        Results On What People Need to Trust AI
      </p>
    </div>
  </header>

  <!-- MAIN ARTICLE -->
  <div class="flex justify-center w-full px-4">
    <main class="article-content w-full p-6 bg-gray-800 rounded-xl shadow-2xl">

      <p class="text-lg mb-6 leading-relaxed">
        In an Emotion Encoded survey, I asked 66 people:
      </p>

      <blockquote class="border-l-4 border-orange-400 pl-4 py-2 my-6 italic text-xl text-gray-300 bg-gray-700 p-4 rounded-lg">
        ‚ÄúWhat would you need to know about an AI tool in order to trust it more?‚Äù
      </blockquote>

      <p class="text-lg mb-8 leading-relaxed">
        The responses to this question highlight that trust in AI is not automatic. It must be earned. People do not simply accept technology because it works‚Äîthey demand conditions that make them feel secure, informed, and respected.
      </p>

      <ul class="space-y-4 list-disc list-inside text-lg">
        <li>
          <strong class="text-orange-300">47%</strong> want clear information about the data the AI was trained on. People care about the foundation of the system, what it has learned from, because this determines whether it can be biased, fair, or representative of reality.
        </li>
        <li>
          <strong class="text-orange-300">45.5%</strong> want details about how the AI works. They do not necessarily need to read code, but they want a digestible explanation of the logic and mechanics behind decisions.
        </li>
        <li>
          <strong class="text-orange-300">45.5%</strong> also want to know how often the AI makes mistakes. Accuracy alone is not enough. Users want transparency about failure rates and limitations, not just success stories.
        </li>
        <li>
          <strong class="text-orange-300">43.9%</strong> said they would trust AI more if a human was still involved in the process. This suggests that many people still need the reassurance of human judgment, especially when outcomes directly affect lives.
        </li>
        <li>
          <strong class="text-orange-300">21.2%</strong> cared most about who created the AI. This shows that the brand behind a tool is less important than how it performs and explains itself.
        </li>
        <li>
          <strong class="text-orange-300">22.7%</strong> admitted they could ‚Äúnever fully trust AI,‚Äù revealing a baseline skepticism that may never be overcome, no matter the design.
        </li>
      </ul>

      <p class="text-lg mt-8 leading-relaxed">
        Tech companies have tried to address these fears by promoting Explainable AI (XAI). This is a set of tools and frameworks designed to clarify AI decision-making and reduce the fear of the black box. Influential tech giants and policymakers have treated XAI as the cure to public distrust. The reality, however, is more complex. Simply offering algorithmic explanations does not automatically create trust.
      </p>

      <p class="text-lg mt-6 leading-relaxed">
        From a psychological perspective, trust in AI is less about liking machines and more about reducing uncertainty. Humans are naturally risk averse. We build trust in one another by showing consistency, reliability, and accountability over time. The same principle applies to AI. People feel safer when they know how systems make decisions, when they have evidence of accuracy and error rates, and when they see that a human can intervene if necessary.
      </p>

      <p class="text-lg mt-6 leading-relaxed">
        These findings echo a deeper truth. Trust is built through transparency, accountability, and reliability, not marketing or promises of perfection. Without these qualities, even the most statistically accurate AI systems risk rejection. Once rejected, the potential benefits of the technology in healthcare, law, or education can be lost entirely.
      </p>

      <p class="text-lg mt-6 leading-relaxed">
        Ultimately, building trust in AI is not just a technical challenge but also a psychological one. Trust must be earned through openness, through admitting limitations, and through respecting the human need for clarity and control. AI that fails to meet these conditions will continue to face resistance, no matter how advanced it becomes.
      </p>

    </main>
  </div>

  <!-- NAVIGATION -->
  <nav class="fixed bottom-0 left-0 right-0 py-4 px-6 bg-gray-900 border-t border-gray-700 shadow-2xl z-50">
            <a href="index.html" class="text-white hover:text-orange-400 transition-colors duration-200 transform hover:scale-105 glow-hover">Home</a>
            <a href="about.html" class="text-white hover:text-orange-400 transition-colors duration-200 transform hover:scale-105 glow-hover">About</a>
            <a href="Methodsanddata.html" class="text-white hover:text-orange-400 transition-colors duration-200 transform hover:scale-105 glow-hover">Methods and Data</a>
            <a href="DataAnalysisquantitative.html" class="text-white hover:text-orange-400 transition-colors duration-200 transform hover:scale-105 glow-hover">Publications</a>
            <a href="ethicalguidelines.html" class="text-white hover:text-orange-400 transition-colors duration-200 transform hover:scale-105 glow-hover">Ethical Guidelines</a>
            <a href="AI-Perception-Briefs.html" class="text-white hover:text-orange-400 transition-colors duration-200 transform hover:scale-105 glow-hover">Research Findings Articles</a>
            <a href="contact.html" class="text-white hover:text-orange-400 transition-colors duration-200 transform hover:scale-105 glow-hover">Contact</a>
        </nav>
    </header>
    </div>

  <!-- FOOTER -->
  <footer class="text-center text-sm text-gray-500 py-4 mt-8">
    ¬© 2025 Emotion Encoded ‚Äî All rights reserved.
  </footer>

</body>
</html>
