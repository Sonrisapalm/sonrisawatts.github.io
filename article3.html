<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Article 3 | Emotion Encoded</title>
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="style.css" />
</head>
<body>

  <!-- Navigation -->
  <nav>
    <nav class="absolute bottom-0 left-0 right-0 py-4 px-6 md:px-12 flex flex-wrap justify-center md:space-x-8 space-x-4 text-base md:text-xl lg:text-2xl">
            <a href="index.html" class="text-white hover:text-orange-400 transition-colors duration-200 transform hover:scale-105 glow-hover">Home</a>
            <a href="about.html" class="text-white hover:text-orange-400 transition-colors duration-200 transform hover:scale-105 glow-hover">About</a>
            <a href="Methodsanddata.html" class="text-white hover:text-orange-400 transition-colors duration-200 transform hover:scale-105 glow-hover">Methods and Data</a>
            <a href="ethicalguidelines.html" class="text-white hover:text-orange-400 transition-colors duration-200 transform hover:scale-105 glow-hover">Ethical Guidelines</a>
            <a href="AI-Perception-Briefs.html" class="text-white hover:text-orange-400 transition-colors duration-200 transform hover:scale-105 glow-hover">Research Findings Articles</a>
            <a href="contact.html" class="text-white hover:text-orange-400 transition-colors duration-200 transform hover:scale-105 glow-hover">Contact</a>
        </nav>
    </header>
  </nav>

  <!-- Header -->
  <header>
    <img src="images/article3.jpg" alt="AI Trust Survey Illustration" class="hero-image" />
    <h1>üëè St. Kitts & Nevis üëè</h1>
    <p>Results On What People Need to Trust AI</p>
  </header>

  <!-- ‚úÖ Background wrapper -->
  <div class="full-width-bg">
    <main class="article-content">
      <p>In an Emotion Encoded survey, I asked 66 people:</p>
      <blockquote>‚ÄúWhat would you need to know about an AI tool in order to trust it more?‚Äù</blockquote>

      <p>The responses to this question highlight that trust in AI is not automatic. It must be earned. People do not simply accept technology because it works, they demand conditions that make them feel secure, informed, and respected.</p>

      <ul>
        <li><strong>47%</strong> want clear information about the data the AI was trained on. People care about the foundation of the system, what it has learned from, because this determines whether it can be biased, fair, or representative of reality.</li>
        <li><strong>45.5%</strong> want details about how the AI works. They do not necessarily need to read code, but they want a digestible explanation of the logic and mechanics behind decisions.</li>
        <li><strong>45.5%</strong> also want to know how often the AI makes mistakes. Accuracy alone is not enough. Users want transparency about failure rates and limitations, not just success stories.</li>
        <li><strong>43.9%</strong> said they would trust AI more if a human was still involved in the process. This suggests that many people still need the reassurance of human judgment, especially when outcomes directly affect lives.</li>
        <li><strong>21.2%</strong> cared most about who created the AI. This shows that the brand behind a tool is less important than how it performs and explains itself.</li>
        <li><strong>22.7%</strong> admitted they could ‚Äúnever fully trust AI,‚Äù revealing a baseline skepticism that may never be overcome, no matter the design.</li>
      </ul>

      <p>
        Tech companies have tried to address these fears by promoting Explainable AI (XAI). This is a set of tools and frameworks designed to clarify AI decision-making and reduce the fear of the black box. Influential tech giants and policymakers have treated XAI as the cure to public distrust. The reality, however, is more complex. Simply offering algorithmic explanations does not automatically create trust.
      </p>

      <p>
        From a psychological perspective, trust in AI is less about liking machines and more about reducing uncertainty. Humans are naturally risk averse. We build trust in one another by showing consistency, reliability, and accountability over time. The same principle applies to AI. People feel safer when they know how systems make decisions, when they have evidence of accuracy and error rates, and when they see that a human can intervene if necessary.
      </p>

      <p>
        These findings echo a deeper truth. Trust is built through transparency, accountability, and reliability, not marketing or promises of perfection. Without these qualities, even the most statistically accurate AI systems risk rejection. Once rejected, the potential benefits of the technology in healthcare, law, or education can be lost entirely.
      </p>

      <p>
        Ultimately, building trust in AI is not just a technical challenge but also a psychological one. Trust must be earned through openness, through admitting limitations, and through respecting the human need for clarity and control. AI that fails to meet these conditions will continue to face resistance, no matter how advanced it becomes.
      </p>
    </main>
  </div>

  <!-- Footer -->
  <footer>
    ¬© 2025 Emotion Encoded ‚Äî All rights reserved.
  </footer>

</body>
</html>
